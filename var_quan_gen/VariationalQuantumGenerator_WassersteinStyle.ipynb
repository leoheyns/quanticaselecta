{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Quantum Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister, transpile\n",
    "from qiskit.tools.visualization import circuit_drawer\n",
    "from qiskit.quantum_info import state_fidelity\n",
    "from qiskit import BasicAer, Aer\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow import keras as ks\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    \n",
    "    def __init__(self, n_points=500):\n",
    "        mu_1 = 0.20\n",
    "        sigma_1 = 0.03\n",
    "        mu_2 = 0.40\n",
    "        sigma_2 = 0.04\n",
    "        self.n_points = n_points\n",
    "        self.distribution = np.append(np.random.normal(mu_1, sigma_1, int(self.n_points / 2)), np.random.normal(mu_2, sigma_2, int(self.n_points / 2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipConstraint(ks.constraints.Constraint):\n",
    "    # set clip value when initialized\n",
    "    def __init__(self, clip_value):\n",
    "        self.clip_value = clip_value\n",
    "\n",
    "    # clip model weights to hypercube\n",
    "    def __call__(self, weights):\n",
    "        return ks.backend.clip(weights, -self.clip_value, self.clip_value)\n",
    "\n",
    "    # get the config\n",
    "    def get_config(self):\n",
    "        return {'clip_value': self.clip_value}\n",
    "    \n",
    "# implementation of wasserstein loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return ks.backend.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "decoding_layer (Dense)       (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class VariationalQuantumGAN():\n",
    "\n",
    "    def __init__(self, n_epochs=50):\n",
    "        super().__init__()\n",
    "        # Initialize parameters\n",
    "        self.backend = Aer.get_backend('statevector_simulator')\n",
    "        self.backend_sim = Aer.get_backend('qasm_simulator')\n",
    "        self.latent_space_size = 2\n",
    "        self.variational_circuit_size = 2\n",
    "        \n",
    "        # Training parameters\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "        # Parameters initialized as in the paper.\n",
    "        self.qg_thetas = [2.3, 2.3, 1.0, 1.0, 1.5, 0.2]\n",
    "        # Build circuit with class parameters TODO: Use this to improve efficiency (if needed)\n",
    "        self.qc = self.build_circuit()\n",
    "        \n",
    "        self.decoding_model = None\n",
    "        self.build_decoding_model()\n",
    "        \n",
    "        self.data_generator = DataGenerator()\n",
    "        self.target_dist = self.data_generator.distribution\n",
    "        \n",
    "        self.discriminator = self.build_discriminator()\n",
    "        \n",
    "        # Build gan stack for fitting the generator\n",
    "        self.gan_stack = ks.Sequential()\n",
    "        self.gan_stack.add(self.decoding_model)\n",
    "        self.gan_stack.add(self.discriminator)\n",
    "        # TODO: Check learning rate\n",
    "        opt = ks.optimizers.RMSprop(lr=0.00005)\n",
    "        self.gan_stack.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "        \n",
    "    def build_circuit(self, measurement=True):\n",
    "        qc = QuantumCircuit(self.variational_circuit_size, self.variational_circuit_size)\n",
    "    \n",
    "        # Quantum encoding.\n",
    "        # z ~ U[-1, 1]\n",
    "        z = np.random.uniform(-1, 1, self.latent_space_size)\n",
    "\n",
    "        for i in range(self.latent_space_size):\n",
    "            qc.rx(1/math.sin(z[i]), i + (self.variational_circuit_size - self.latent_space_size))\n",
    "            qc.rz(1/math.cos(z[i]), i + (self.variational_circuit_size - self.latent_space_size))\n",
    "\n",
    "        qc.barrier()\n",
    "\n",
    "        # Variational Circuit\n",
    "        # TODO: Scale dynamically with circuit size?\n",
    "        qc.ry(self.qg_thetas[0], 0)\n",
    "        qc.ry(self.qg_thetas[1], 1)\n",
    "        qc.rxx(self.qg_thetas[2], 0, 1)\n",
    "        qc.barrier()\n",
    "        \n",
    "        qc.ry(self.qg_thetas[3], 0)\n",
    "        qc.ry(self.qg_thetas[4], 1)\n",
    "        qc.rxx(self.qg_thetas[5], 0, 1)\n",
    "        qc.barrier()\n",
    "\n",
    "        # Measurement decoding\n",
    "        if measurement:\n",
    "            qc.measure(0, 0)\n",
    "            qc.measure(1, 1)\n",
    "        \n",
    "        return qc\n",
    "    \n",
    "    # Build circuit with params as arguments, used for parameter shift gradient calculation\n",
    "    def build_circuit_with_params(self, params, measurement=True):\n",
    "        qc = QuantumCircuit(self.variational_circuit_size, self.variational_circuit_size)\n",
    "    \n",
    "        # Quantum encoding.\n",
    "        # z ~ U[-1, 1]\n",
    "        z = np.random.uniform(-1, 1, self.latent_space_size)\n",
    "\n",
    "        for i in range(self.latent_space_size):\n",
    "            qc.rx(1/math.sin(z[i]), i + (self.variational_circuit_size - self.latent_space_size))\n",
    "            qc.rz(1/math.cos(z[i]), i + (self.variational_circuit_size - self.latent_space_size))\n",
    "\n",
    "        qc.barrier()\n",
    "\n",
    "        # Variational Circuit\n",
    "        # TODO: Scale dynamically with circuit size?\n",
    "        qc.ry(params[0], 0)\n",
    "        qc.ry(params[1], 1)\n",
    "        qc.rxx(params[2], 0, 1)\n",
    "        qc.barrier()\n",
    "        \n",
    "        qc.ry(params[3], 0)\n",
    "        qc.ry(params[4], 1)\n",
    "        qc.rxx(params[5], 0, 1)\n",
    "        qc.barrier()\n",
    "\n",
    "        # Measurement decoding\n",
    "        if measurement:\n",
    "            qc.measure(0, 0)\n",
    "            qc.measure(1, 1)\n",
    "        \n",
    "        return qc\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        # define the constraint\n",
    "        const = ClipConstraint(0.01)\n",
    "        \n",
    "        model = ks.Sequential()\n",
    "        model.add(ks.layers.Dense(50, activation='elu', input_shape=(1,), kernel_constraint=const))\n",
    "#         model.add(ks.layers.Dense(100, activation='relu', kernel_constraint=const))\n",
    "        model.add(ks.layers.Dense(50, activation='elu', kernel_constraint=const))\n",
    "        model.add(ks.layers.Dense(1, activation='linear', kernel_constraint=const))\n",
    "        \n",
    "        # TODO: Investigate loss\n",
    "        opt = ks.optimizers.RMSprop(lr=0.005)\n",
    "        model.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "        return model\n",
    "    \n",
    "    def build_decoding_model(self):\n",
    "        x = ks.layers.Input(shape=(self.variational_circuit_size,), name=\"input\")\n",
    "        decoding_layer = ks.layers.Dense(1, activation=\"sigmoid\", name=\"decoding_layer\")\n",
    "        y = decoding_layer(x)\n",
    "\n",
    "        self.decoding_model = ks.Model(inputs=x, outputs=y)\n",
    "        self.decoding_model.summary()\n",
    "        \n",
    "    # Generate measurement with the class \n",
    "    def generate_measurement(self, qc=None, statevector=False):\n",
    "        # Somehow statevector is slower: TODO figure out. 25 seconds sv vs 15 seconds 100 shot sims.\n",
    "        if not statevector:\n",
    "            # TODO: change to use a class qc\n",
    "            if qc is None:\n",
    "                qc = self.build_circuit()\n",
    "            sim_shots = 100\n",
    "\n",
    "            job = self.backend_sim.run(qc, shots=sim_shots)\n",
    "            result_sim = job.result()\n",
    "\n",
    "            counts = result_sim.get_counts(qc)\n",
    "    #         print(counts)\n",
    "\n",
    "            classical_latent_space = np.zeros(self.variational_circuit_size)\n",
    "            for count in counts.keys():\n",
    "                v_n = 0\n",
    "                for bit in count:\n",
    "                    if bit == '1':\n",
    "                        classical_latent_space[v_n] += counts[count]\n",
    "                    v_n += 1\n",
    "\n",
    "            classical_latent_space = np.stack([classical_latent_space/sim_shots])\n",
    "\n",
    "            return classical_latent_space\n",
    "        \n",
    "        else:\n",
    "            if qc is None:\n",
    "                qc_no_measure = self.build_circuit(measurement=False)\n",
    "                qc_no_measure.save_statevector()\n",
    "            else:\n",
    "                qc_no_measure = qc\n",
    "            \n",
    "\n",
    "            # Transpile for simulator\n",
    "            simulator = Aer.get_backend('aer_simulator')\n",
    "            circ = transpile(qc_no_measure, simulator)\n",
    "\n",
    "            # Run and get statevector\n",
    "            result = simulator.run(circ).result()\n",
    "            statevector = result.get_statevector(circ)\n",
    "\n",
    "            collapsed_sv = np.abs(np.stack(statevector)) ** 2\n",
    "\n",
    "            classical_latent_space = np.zeros(self.variational_circuit_size)\n",
    "            for i in range(len(collapsed_sv)):\n",
    "                # Counts are fixed. CHANGE 02b TO NEW LATENT SPACE SIZE IF CHANGED\n",
    "                current_count = \"{0:02b}\".format(i)\n",
    "                for bit in range(len(current_count)):\n",
    "                    if current_count[bit] == \"1\":\n",
    "                        classical_latent_space[bit] += collapsed_sv[i]\n",
    "\n",
    "    #             000 , 001, 010, 011, 100, 101, 110, 111\n",
    "\n",
    "    #         print(classical_latent_space)\n",
    "    #         print(collapsed_sv)\n",
    "\n",
    "            classical_latent_space = classical_latent_space.reshape(1, self.variational_circuit_size)\n",
    "\n",
    "            return classical_latent_space\n",
    "        \n",
    "    def generate_prediction(self, qc=None):\n",
    "        circuit_measurement = self.generate_measurement(qc=qc)\n",
    "        return self.decoding_model.predict(circuit_measurement)\n",
    "    \n",
    "    def generate_fake_samples(self, n):\n",
    "        print(\"Generating fake samples for training\")\n",
    "        predictions = []\n",
    "#         self.printProgressBar(iteration=0, total=10)\n",
    "        j = 0\n",
    "        for i in range(n):\n",
    "            predictions.append(self.generate_prediction())\n",
    "#             if i%(int(n/10)) == 0:\n",
    "#                 j += 1\n",
    "#                 self.printProgressBar(iteration=j, total=10)\n",
    "            \n",
    "        predictions = np.stack(predictions)\n",
    "        predictions = predictions.reshape((n, 1))\n",
    "        \n",
    "        return predictions, np.ones((n, 1))\n",
    "    \n",
    "    def generate_fake_samples(self, n, qc=None):\n",
    "        print(\"Generating fake samples for training\")\n",
    "        predictions = []\n",
    "#         self.printProgressBar(iteration=0, total=10)\n",
    "        j = 0\n",
    "        for i in range(n):\n",
    "            predictions.append(self.generate_prediction(qc=qc))\n",
    "#             if i%(int(n/10)) == 0:\n",
    "#                 j += 1\n",
    "#                 self.printProgressBar(iteration=j, total=10)\n",
    "            \n",
    "        predictions = np.stack(predictions)\n",
    "        predictions = predictions.reshape((n, 1))\n",
    "        \n",
    "        return predictions, np.ones((n, 1))\n",
    "    \n",
    "    def train(self):\n",
    "        self.discriminator.summary()\n",
    "        \n",
    "        # TODO: Resample real distribution continuously instead of using fixed amount of points\n",
    "        X_real = np.stack(self.target_dist).reshape(len(self.target_dist), 1)\n",
    "        y_real = -np.ones((len(self.target_dist), 1))\n",
    "        \n",
    "        n_critic = 10\n",
    "        gen_batch_size = 32\n",
    "        disc_batch_size = gen_batch_size * n_critic * 2\n",
    "        \n",
    "        for i in range(self.n_epochs):\n",
    "            X_fake_sampled, y_fake_sampled = self.generate_fake_samples(int(disc_batch_size / 2))\n",
    "            # Train discriminator\n",
    "            # TODO: Perform basic improvements on discriminator\n",
    "            # TODO: Create wasserstein critic\n",
    "            \n",
    "            # Select read and fake data_points, we assume N_fake == N_real (Amount fake datapoints is equal to amount of real datapoints)\n",
    "            idx = np.random.choice(np.arange(len(X_real)), int(disc_batch_size / 2))\n",
    "            \n",
    "            # Train on real\n",
    "            X_real_sampled, y_real_sampled = np.take(X_real, idx), np.take(y_real, idx)\n",
    "            \n",
    "#             preds = self.discriminator.predict_on_batch\n",
    "            self.discriminator.train_on_batch(X_real_sampled, y_real_sampled)\n",
    "            \n",
    "            # Train on fake\n",
    "            self.discriminator.train_on_batch(X_fake_sampled, y_fake_sampled)\n",
    "            \n",
    "            print(\"Iteration\", i)#, \"Real accuracy\", acc_real, \"Fake accuracy\", acc_fake)\n",
    "            \n",
    "            # Train generator stack\n",
    "            # TODO: Figure out gradients exactly\n",
    "            \n",
    "            # Calculate Generator Cost\n",
    "            disc_pred = 0\n",
    "            for j in range(len(X_fake_sampled)):\n",
    "                disc_pred += self.discriminator.predict(X_fake_sampled[j])/disc_batch_size\n",
    "                \n",
    "            print(\"Gen cost:\", disc_pred)\n",
    "            self.generate_critic_belief(plot=True, iteration=i)\n",
    "            \n",
    "            self.discriminator.trainable = False\n",
    "            # Update Quantum and Classical part of generator with parameter shift\n",
    "            for j in range(gen_batch_size):\n",
    "                non_shift_qc = self.build_circuit_with_params(self.qg_thetas)\n",
    "                non_shift_measurement = self.generate_measurement(qc=non_shift_qc)\n",
    "#                 print(non_shift_measurement)\n",
    "                non_shift_sample = self.decoding_model.predict(non_shift_measurement)\n",
    "                non_shift_cost = self.discriminator.predict([non_shift_sample])\n",
    "                \n",
    "                # TODO: Check .astype float for tf retracing inefficiency\n",
    "                self.gan_stack.train_on_batch(np.stack(non_shift_measurement), np.stack([1]).astype(float))\n",
    "                \n",
    "                for k in range(len(self.qg_thetas)):\n",
    "                    neg_shift_params = self.qg_thetas.copy()\n",
    "                    neg_shift_params[k] -= math.pi/2\n",
    "                    neg_shift_qc = self.build_circuit_with_params(neg_shift_params)\n",
    "                    \n",
    "                    neg_shift_sample = self.generate_prediction(qc=neg_shift_qc)\n",
    "                    neg_shift_cost = self.discriminator.predict([neg_shift_sample])\n",
    "                    \n",
    "                    pos_shift_params = self.qg_thetas.copy()\n",
    "                    pos_shift_params[k] -= math.pi/2\n",
    "                    pos_shift_qc = self.build_circuit_with_params(pos_shift_params)\n",
    "                    \n",
    "                    pos_shift_sample = self.generate_prediction(qc=pos_shift_qc)\n",
    "                    pos_shift_cost = self.discriminator.predict([pos_shift_sample])\n",
    "                    \n",
    "                    gradient = pos_shift_cost - neg_shift_cost\n",
    "                    \n",
    "                    self.qg_thetas[k] += gradient * non_shift_cost\n",
    "            \n",
    "            \n",
    "            self.discriminator.trainable = True\n",
    "            \n",
    "            if i % 5 == 0:\n",
    "                self.compare_model_to_real()\n",
    "                \n",
    "    def generate_critic_belief(self, plot=False, iteration=-1):\n",
    "        values = np.arange(0, 1, 0.01)\n",
    "        beliefs = []\n",
    "        for i in range(len(values)):\n",
    "            beliefs.append(self.discriminator.predict([values[i]]))\n",
    "        \n",
    "        beliefs = np.reshape(beliefs, (100))\n",
    "        \n",
    "        if plot:\n",
    "            plt.plot(np.arange(0, 1, 0.01), beliefs, label=\"belief\")\n",
    "            plt.title(\"Belief at iteration \" + str(iteration))\n",
    "#             plt.ylim(-1, 1)\n",
    "            plt.show()\n",
    "            \n",
    "        return beliefs\n",
    "            \n",
    "    def compare_model_to_real(self):\n",
    "        bins = np.linspace(0, 1, 100)\n",
    "        \n",
    "        plt.hist(self.target_dist, bins, density=True, label=\"True\")\n",
    "        predictions = []\n",
    "        self.printProgressBar(iteration=0, total=10)\n",
    "        j = 0\n",
    "        for i in range(self.data_generator.n_points):\n",
    "            predictions.append(self.generate_prediction())\n",
    "            if i%(self.data_generator.n_points/10) == 0:\n",
    "                j += 1\n",
    "                self.printProgressBar(iteration=j, total=10)\n",
    "#                 print(str(i/self.data_generator.n_points*100) + \"%\")\n",
    "                \n",
    "        predictions = np.stack(predictions)\n",
    "        predictions = predictions.reshape((self.data_generator.n_points,))\n",
    "\n",
    "        plt.hist(predictions, bins, density=True, label=\"False\")\n",
    "        \n",
    "        plt.plot(np.arange(0, 1, 0.01), self.generate_critic_belief(), label=\"Critic belief\")\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def printProgressBar (self, iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "        percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "        filledLength = int(length * iteration // total)\n",
    "        bar = fill * filledLength + '-' * (length - filledLength)\n",
    "        print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "        # Print New Line on Complete\n",
    "        if iteration == total: \n",
    "            print()\n",
    "            \n",
    "    \n",
    "vqgan = VariationalQuantumGAN()\n",
    "# vqgan.train()\n",
    "# vqgan.generate_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">      ┌────────────┐┌───────────┐  ░ ┌─────────┐┌─────────┐ ░  ┌───────┐ »\n",
       "q_0: ─┤ Rx(-1.508) ├┤ Rz(1.336) ├──░─┤ Ry(2.3) ├┤0        ├─░──┤ Ry(1) ├─»\n",
       "     ┌┴────────────┤├───────────┴┐ ░ ├─────────┤│  Rxx(1) │ ░ ┌┴───────┴┐»\n",
       "q_1: ┤ Rx(-7.0049) ├┤ Rz(1.0103) ├─░─┤ Ry(2.3) ├┤1        ├─░─┤ Ry(1.5) ├»\n",
       "     └─────────────┘└────────────┘ ░ └─────────┘└─────────┘ ░ └─────────┘»\n",
       "c: 2/════════════════════════════════════════════════════════════════════»\n",
       "                                                                         »\n",
       "«     ┌───────────┐ ░ ┌─┐   \n",
       "«q_0: ┤0          ├─░─┤M├───\n",
       "«     │  Rxx(0.2) │ ░ └╥┘┌─┐\n",
       "«q_1: ┤1          ├─░──╫─┤M├\n",
       "«     └───────────┘ ░  ║ └╥┘\n",
       "«c: 2/═════════════════╩══╩═\n",
       "«                      0  1 </pre>"
      ],
      "text/plain": [
       "      ┌────────────┐┌───────────┐  ░ ┌─────────┐┌─────────┐ ░  ┌───────┐ »\n",
       "q_0: ─┤ Rx(-1.508) ├┤ Rz(1.336) ├──░─┤ Ry(2.3) ├┤0        ├─░──┤ Ry(1) ├─»\n",
       "     ┌┴────────────┤├───────────┴┐ ░ ├─────────┤│  Rxx(1) │ ░ ┌┴───────┴┐»\n",
       "q_1: ┤ Rx(-7.0049) ├┤ Rz(1.0103) ├─░─┤ Ry(2.3) ├┤1        ├─░─┤ Ry(1.5) ├»\n",
       "     └─────────────┘└────────────┘ ░ └─────────┘└─────────┘ ░ └─────────┘»\n",
       "c: 2/════════════════════════════════════════════════════════════════════»\n",
       "                                                                         »\n",
       "«     ┌───────────┐ ░ ┌─┐   \n",
       "«q_0: ┤0          ├─░─┤M├───\n",
       "«     │  Rxx(0.2) │ ░ └╥┘┌─┐\n",
       "«q_1: ┤1          ├─░──╫─┤M├\n",
       "«     └───────────┘ ░  ║ └╥┘\n",
       "«c: 2/═════════════════╩══╩═\n",
       "«                      0  1 "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "latent_space_size = 2\n",
    "variational_circuit_size = 2\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 5\n",
    "\n",
    "# Parameters initialized as in the paper.\n",
    "qg_thetas = [2.3, 2.3, 1.0, 1.0, 1.5, 0.2]\n",
    "\n",
    "qc = QuantumCircuit(variational_circuit_size, variational_circuit_size)\n",
    "\n",
    "# Quantum encoding.\n",
    "# z ~ U[-1, 1]\n",
    "z = np.random.uniform(-1, 1, latent_space_size)\n",
    "\n",
    "for i in range(latent_space_size):\n",
    "    qc.rx(1/math.sin(z[i]), i + (variational_circuit_size - latent_space_size))\n",
    "    qc.rz(1/math.cos(z[i]), i + (variational_circuit_size - latent_space_size))\n",
    "\n",
    "qc.barrier()\n",
    "\n",
    "# Variational Circuit\n",
    "# TODO: Scale dynamically with circuit size?\n",
    "qc.ry(qg_thetas[0], 0)\n",
    "qc.ry(qg_thetas[1], 1)\n",
    "qc.rxx(qg_thetas[2], 0, 1)\n",
    "qc.barrier()\n",
    "\n",
    "qc.ry(qg_thetas[3], 0)\n",
    "qc.ry(qg_thetas[4], 1)\n",
    "qc.rxx(qg_thetas[5], 0, 1)\n",
    "qc.barrier()\n",
    "\n",
    "# Measurement decoding\n",
    "qc.measure(0, 0)\n",
    "qc.measure(1, 1)\n",
    "\n",
    "qc.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
